import logging
import json
from pathlib import Path
import geopandas as gpd
import pandas as pd

# Local imports
from src.config import AppConfig
from src.utils import (
    display_multi_layer_on_folium_map,
    load_vector_data,
    # Import other display functions if needed, or rely on multi_layer
)

logger = logging.getLogger(__name__)


class EvaluateTask:
    """
    Task to evaluate the results of the MCA workflow.
    - Generates visualizations for all intermediate and final feature layers.
    - Ranks the final recommended segments (Overlay D) based on criteria.
    - Saves metadata about the run.
    """

    # Define ranking criteria and corresponding column names
    # Assuming lower cost is better, higher others are better
    RANKING_CRITERIA = {
        "popularity": {"column": "efforts_per_age", "ascending": False},
        "speed": {"column": "avg_speed", "ascending": False},
        "traffic": {"column": "avg_traffic", "ascending": False},
        "cost": {"column": "avg_cost", "ascending": True},
    }

    def __init__(self, settings: AppConfig, feature_outputs: dict, combined_outputs: dict):
        """
        Initializes the EvaluateTask.

        Args:
            settings: The application configuration.
            feature_outputs: Dictionary of outputs from the build_features task.
            combined_outputs: Dictionary of outputs from the combine_features task.
        """
        self.settings = settings
        self.feature_outputs = feature_outputs
        self.combined_outputs = combined_outputs
        self.output_dir = settings.paths.output_dir
        self.output_paths = {} # To store paths generated by this task

    def _get_output_path(self, key: str, suffix: str = ".html") -> Path:
        """Helper to construct an output path within the evaluation task's context."""
        # Use a sub-directory within the main output for evaluation results
        eval_output_dir = self.output_dir / "evaluation"
        eval_output_dir.mkdir(parents=True, exist_ok=True)
        # Use Pydantic model attribute name directly if available, otherwise use key
        filename_base = getattr(self.settings.output_files, key, key)
        # Remove existing suffix if present before adding the new one
        filename_base = Path(filename_base).stem
        return eval_output_dir / f"{filename_base}{suffix}"

    def _run_single_display(self, display_func, layers_config, output_key):
        """Runs a display function and stores the output path."""
        output_html_path = self._get_output_path(output_key)
        try:
            logger.info(f"Generating display for: {output_key}")
            map_path = display_func(
                layers=layers_config,
                output_html_path_str=str(output_html_path),
                map_zoom=11, # Adjust as needed
                map_tiles='CartoDB positron'
            )
            if map_path:
                self.output_paths[output_key] = Path(map_path)
                logger.info(f"Successfully generated display: {map_path}")
            else:
                logger.warning(f"Display function did not return a path for {output_key}.")
        except Exception as e:
            logger.error(f"Error generating display for {output_key}: {e}", exc_info=True)


    def run_displays(self):
        """Generates visualizations for all relevant feature layers."""
        logger.info("--- Generating Feature Visualizations ---")

        # --- Cost Layer Display ---
        cost_raster = self.feature_outputs.get("cost_raster")
        slope_raster = self.feature_outputs.get("slope_raster") # Assuming slope is output by elevation
        speed_raster = self.feature_outputs.get("speed_raster") # Assuming speed is output by heatmap
        if cost_raster:
            cost_layers = [
                {'path': cost_raster, 'name': 'Normalized Cost', 'type': 'raster', 'raster': {'cmap': 'viridis_r', 'show': True}},
                {'path': slope_raster, 'name': 'Slope', 'type': 'raster', 'raster': {'cmap': 'terrain', 'show': False}} if slope_raster else None,
                {'path': speed_raster, 'name': 'Avg Speed', 'type': 'raster', 'raster': {'cmap': 'plasma', 'show': False}} if speed_raster else None,
            ]
            self._run_single_display(display_multi_layer_on_folium_map, [l for l in cost_layers if l], "cost_visualization")
        else:
            logger.warning("Skipping Cost display: Cost raster path not found.")

        # --- Elevation Layer Display ---
        dem_raster = self.feature_outputs.get("dem_raster")
        # slope_raster already retrieved above
        if dem_raster and slope_raster:
            elevation_layers = [
                {'path': dem_raster, 'name': 'DEM', 'type': 'raster', 'raster': {'cmap': 'terrain', 'show': True}},
                {'path': slope_raster, 'name': 'Slope', 'type': 'raster', 'raster': {'cmap': 'coolwarm', 'show': False}},
            ]
            self._run_single_display(display_multi_layer_on_folium_map, elevation_layers, "elevation_visualization")
        else:
            logger.warning("Skipping Elevation display: DEM or Slope raster path not found.")

        # --- Heatmap (Speed) Layer Display ---
        # speed_raster already retrieved above
        # Also need train/test points if we want the same display as in heatmap.py main block
        # For evaluation, maybe just the raster is sufficient?
        if speed_raster:
             heatmap_layers = [
                 {'path': speed_raster, 'name': 'Avg Speed (Heatmap)', 'type': 'raster', 'raster': {'cmap': 'plasma', 'show': True}},
                 # Add train/test points layers if paths are available and needed
             ]
             self._run_single_display(display_multi_layer_on_folium_map, heatmap_layers, "heatmap_visualization")
        else:
             logger.warning("Skipping Heatmap display: Speed raster path not found.")

        # --- Roads Layer Display ---
        road_vectors = self.feature_outputs.get("road_vectors", {})
        roads_layers = []
        road_layer_configs = {
            "samferdsel_all": {"name": "All Roads (Samferdsel)", "color": "gray", "show": False},
            "bike_lanes_filtered": {"name": "Bike Lanes", "color": "blue", "show": True},
            "roads_simple_filtered": {"name": "Simple Roads", "color": "red", "show": False},
            "roads_simple_diff_lanes": {"name": "Simple Roads (No Lanes)", "color": "orange", "show": True},
        }
        for key, config in road_layer_configs.items():
            path = road_vectors.get(key)
            if path and Path(path).exists():
                roads_layers.append({
                    'path': path,
                    'name': config["name"],
                    'type': 'vector',
                    'vector': {'color': config["color"], 'weight': 2, 'show': config["show"]}
                })
            else:
                logger.warning(f"Skipping road layer '{config['name']}': Path not found or invalid.")

        if roads_layers:
            self._run_single_display(display_multi_layer_on_folium_map, roads_layers, "roads_visualization")
        else:
            logger.warning("Skipping Roads display: No valid road vector paths found.")

        # --- Segments (Popularity) Layer Display ---
        segment_vectors = self.feature_outputs.get("segment_vectors", []) # Expecting a list of paths
        if segment_vectors:
            segment_layers = []
            first_segment_layer = True
            for path in segment_vectors:
                path_obj = Path(path)
                if path_obj.exists():
                    metric_name = path_obj.stem.split('vector_')[-1] # Extract metric name
                    # Need to load GDF to find norm column, or make assumption
                    norm_col = f"{metric_name}_norm" # Assume norm column exists
                    segment_layers.append({
                        'path': path,
                        'name': f'Segments ({metric_name})',
                        'type': 'vector',
                        'vector': {
                            'style_column': norm_col, # Use normalized column for styling
                            'cmap': 'viridis',
                            'line_weight': 3,
                            'tooltip_cols': [self.settings.input_data.segment_id_field, metric_name, norm_col],
                            'show': first_segment_layer # Show only the first metric by default
                        }
                    })
                    first_segment_layer = False
                else:
                    logger.warning(f"Skipping segment layer: Path not found {path}")
            if segment_layers:
                 self._run_single_display(display_multi_layer_on_folium_map, segment_layers, "segments_visualization")
            else:
                 logger.warning("Skipping Segments display: No valid segment vector paths found.")
        else:
            logger.warning("Skipping Segments display: No segment vector paths found in feature outputs.")


        # --- Traffic Layer Display ---
        traffic_rasters = self.feature_outputs.get("traffic_rasters", []) # Expecting a list of paths
        if traffic_rasters:
            traffic_layers = []
            first_traffic_layer = True
            for path in traffic_rasters:
                 path_obj = Path(path)
                 if path_obj.exists():
                     period_name = path_obj.stem.split('raster_')[-1] # Extract period name
                     traffic_layers.append({
                         'path': path,
                         'name': f'Traffic ({period_name.capitalize()})',
                         'type': 'raster',
                         'raster': {'cmap': 'plasma', 'show': first_traffic_layer}
                     })
                     first_traffic_layer = False
                 else:
                     logger.warning(f"Skipping traffic layer: Path not found {path}")
            if traffic_layers:
                 self._run_single_display(display_multi_layer_on_folium_map, traffic_layers, "traffic_visualization")
            else:
                 logger.warning("Skipping Traffic display: No valid traffic raster paths found.")
        else:
            logger.warning("Skipping Traffic display: No traffic raster paths found in feature outputs.")

        # --- Overlay D Display ---
        overlay_d_path = self.combined_outputs.get("overlay_d")
        if overlay_d_path and Path(overlay_d_path).exists():
             overlay_d_layers = [{
                 'path': overlay_d_path,
                 'name': 'Overlay D (Final Segments)',
                 'type': 'vector',
                 'vector': {
                     'style_column': 'avg_cost', # Style by final cost? Or a rank?
                     'cmap': 'viridis_r',
                     'line_weight': 4,
                     'tooltip_cols': [ # Show key ranking attributes
                         self.settings.input_data.segment_id_field,
                         self.RANKING_CRITERIA['popularity']['column'],
                         self.RANKING_CRITERIA['speed']['column'],
                         self.RANKING_CRITERIA['traffic']['column'],
                         self.RANKING_CRITERIA['cost']['column'],
                     ],
                     'show': True
                 }
             }]
             self._run_single_display(display_multi_layer_on_folium_map, overlay_d_layers, "overlay_d_visualization")
        else:
             logger.warning("Skipping Overlay D display: Path not found.")


        logger.info("--- Feature Visualizations Generation Finished ---")


    def rank_segments(self):
        """Ranks the segments in Overlay D based on configured criteria."""
        logger.info("--- Ranking Final Segments (Overlay D) ---")
        overlay_d_path = self.combined_outputs.get("overlay_d")
        output_csv_path = self._get_output_path("ranked_segments", suffix=".csv")

        if not overlay_d_path or not Path(overlay_d_path).exists():
            logger.error("Overlay D file not found. Cannot rank segments.")
            return

        try:
            gdf = load_vector_data(overlay_d_path)
            if gdf.empty:
                logger.warning("Overlay D GeoPackage is empty. No segments to rank.")
                # Save empty CSV
                pd.DataFrame().to_csv(output_csv_path, index=False)
                self.output_paths["ranked_segments_csv"] = output_csv_path
                return

            logger.info(f"Loaded {len(gdf)} segments from Overlay D for ranking.")
            logger.debug(f"Overlay D columns: {gdf.columns.tolist()}")

            # --- Prepare for Ranking ---
            ranking_df = gdf.copy()
            # Keep only relevant columns for the final CSV? Or keep all? Keep all for now.
            # relevant_cols = ['geometry', self.settings.input_data.segment_id_field] + \
            #                 [crit['column'] for crit in self.RANKING_CRITERIA.values() if crit['column'] in ranking_df.columns]
            # ranking_df = ranking_df[relevant_cols]

            # --- Calculate Ranks ---
            # Add rank columns for each criterion
            for name, crit in self.RANKING_CRITERIA.items():
                col = crit["column"]
                rank_col = f"{name}_rank"
                if col in ranking_df.columns:
                    # Use 'first' method to handle ties arbitrarily but consistently
                    ranking_df[rank_col] = ranking_df[col].rank(
                        method='first', ascending=crit["ascending"], na_option='bottom'
                    )
                    logger.info(f"Calculated rank for '{name}' based on column '{col}' (Ascending: {crit['ascending']}).")
                else:
                    logger.warning(f"Column '{col}' for ranking criterion '{name}' not found in Overlay D. Skipping rank calculation.")
                    ranking_df[rank_col] = pd.NA # Assign NA if column is missing

            # --- Simple Combined Rank (Average Rank) ---
            # More sophisticated methods (e.g., weighted sum, Borda count) could be used.
            rank_cols = [f"{name}_rank" for name in self.RANKING_CRITERIA if f"{name}_rank" in ranking_df.columns]
            if rank_cols:
                ranking_df['combined_rank_avg'] = ranking_df[rank_cols].mean(axis=1)
                # Rank based on the average rank (lower average is better)
                ranking_df['final_rank'] = ranking_df['combined_rank_avg'].rank(method='first', ascending=True)
                logger.info("Calculated combined average rank and final rank.")
            else:
                logger.warning("No individual rank columns were calculated. Cannot compute combined rank.")
                ranking_df['combined_rank_avg'] = pd.NA
                ranking_df['final_rank'] = pd.NA

            # --- Sort by Final Rank ---
            if 'final_rank' in ranking_df.columns:
                ranked_gdf = ranking_df.sort_values(by='final_rank', ascending=True)
            else:
                ranked_gdf = ranking_df # Return unsorted if ranking failed

            # --- Save to CSV ---
            # Drop geometry for CSV output
            ranked_df_no_geom = ranked_gdf.drop(columns=['geometry'], errors='ignore')
            ranked_df_no_geom.to_csv(output_csv_path, index=False)
            self.output_paths["ranked_segments_csv"] = output_csv_path
            logger.info(f"Ranked segments saved to CSV: {output_csv_path}")

        except Exception as e:
            logger.error(f"Error ranking segments: {e}", exc_info=True)

    def save_metadata(self):
        """Saves metadata about the run configuration."""
        logger.info("--- Saving Run Metadata ---")
        output_json_path = self._get_output_path("run_metadata", suffix=".json")
        try:
            # Convert Pydantic settings model to a dictionary
            # Use model_dump for Pydantic v2+
            if hasattr(self.settings, 'model_dump'):
                 metadata_dict = self.settings.model_dump(mode='json') # Handles Path objects etc.
            else:
                 # Fallback for older Pydantic or other BaseModel types
                 metadata_dict = self.settings.dict()

            # Add timestamp or other run-specific info if desired
            # metadata_dict['evaluation_timestamp'] = datetime.now().isoformat()

            with open(output_json_path, 'w') as f:
                json.dump(metadata_dict, f, indent=4)

            self.output_paths["metadata_json"] = output_json_path
            logger.info(f"Run metadata saved to JSON: {output_json_path}")

        except Exception as e:
            logger.error(f"Error saving metadata: {e}", exc_info=True)


    def run(self) -> dict:
        """
        Executes the evaluation workflow: displays, ranking, metadata.

        Returns:
            Dictionary containing paths to the generated output files (HTML maps,
            ranked CSV, metadata JSON).
        """
        logger.info("====== Starting Evaluation Task ======")
        self.run_displays()
        self.rank_segments()
        self.save_metadata()
        logger.info("====== Evaluation Task Finished ======")
        logger.info("Evaluation Outputs:")
        for key, path in self.output_paths.items():
            logger.info(f"  - {key}: {path}")
        return self.output_paths

# Example Usage (if run standalone, requires dummy inputs)
if __name__ == "__main__":
    from src.config import settings as app_settings # Use loaded settings

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger.info("--- Running evaluate.py Standalone Test ---")

    # --- !!! This requires outputs from previous tasks !!! ---
    # --- Define paths to mock inputs based on a previous run ---
    previous_run_name = "mca_20250426_2235" # ADJUST THIS TO A VALID RUN
    previous_run_dir = app_settings.paths.base_dir / "output" / previous_run_name
    eval_test_output_dir = app_settings.paths.base_dir / "output" / "mca_evaluate_test"
    eval_test_output_dir.mkdir(parents=True, exist_ok=True)
    app_settings.paths.output_dir = eval_test_output_dir # Set output for this test

    logger.info(f"Using inputs from: {previous_run_dir}")
    logger.info(f"Saving evaluation outputs to: {eval_test_output_dir}")

    if not previous_run_dir.is_dir():
        logger.error(f"Previous run directory not found: {previous_run_dir}")
        logger.error("Cannot run evaluation test without inputs.")

    else:
        # --- Mock Inputs (Adjust filenames based on your config/actual outputs) ---
        mock_feature_outputs = {
            "cost_raster": previous_run_dir / app_settings.output_files.calculated_cost_layer,
            "slope_raster": previous_run_dir / app_settings.output_files.slope_raster,
            "dem_raster": previous_run_dir / app_settings.output_files.elevation_dem_raster,
            "speed_raster": previous_run_dir / app_settings.output_files.average_speed_raster,
            "road_vectors": { # Dictionary for road outputs
                 "samferdsel_all": previous_run_dir / app_settings.output_files.prepared_roads_gpkg,
                 "bike_lanes_filtered": previous_run_dir / app_settings.output_files.prepared_kml_bike_lanes_gpkg,
                 "roads_simple_filtered": previous_run_dir / app_settings.output_files.prepared_roads_simple_filtered_gpkg,
                 "roads_simple_diff_lanes": previous_run_dir / app_settings.output_files.prepared_roads_simple_diff_lanes_gpkg,
            },
            "segment_vectors": [ # List for segment popularity vectors
                previous_run_dir / f"{app_settings.output_files.segment_popularity_vector_prefix}_{metric}.gpkg"
                for metric in app_settings.processing.segment_popularity_metrics
            ],
            "traffic_rasters": [ # List for traffic rasters
                 previous_run_dir / app_settings.output_files.traffic_density_raster_morning,
                 previous_run_dir / app_settings.output_files.traffic_density_raster_daytime,
                 previous_run_dir / app_settings.output_files.traffic_density_raster_evening,
            ],
            # Add other outputs from build_features if needed by displays
        }
        mock_combined_outputs = {
            "overlay_a": previous_run_dir / app_settings.output_files.overlay_a_gpkg,
            "overlay_b": previous_run_dir / app_settings.output_files.overlay_b_gpkg,
            "overlay_c": previous_run_dir / app_settings.output_files.overlay_c_gpkg,
            "overlay_d": previous_run_dir / app_settings.output_files.overlay_d_gpkg,
        }

        # --- Verify Mock Inputs Exist ---
        missing_files = []
        for key, val in mock_feature_outputs.items():
            if isinstance(val, dict): # Check nested dicts (e.g., road_vectors)
                for sub_key, sub_val in val.items():
                    if not Path(sub_val).exists(): missing_files.append(f"{key}.{sub_key}: {sub_val}")
            elif isinstance(val, list): # Check lists (e.g., segment_vectors, traffic_rasters)
                 for item_path in val:
                     if not Path(item_path).exists(): missing_files.append(f"{key} item: {item_path}")
            elif not Path(val).exists():
                 missing_files.append(f"{key}: {val}")
        for key, val in mock_combined_outputs.items():
             if not Path(val).exists(): missing_files.append(f"{key}: {val}")

        if missing_files:
            logger.error("Missing required input files for evaluation test:")
            for f in missing_files: logger.error(f" - {f}")
        else:
            # --- Run Evaluation Task ---
            try:
                eval_task = EvaluateTask(app_settings, mock_feature_outputs, mock_combined_outputs)
                eval_results = eval_task.run()
                logger.info("Evaluation task completed.")
                logger.info("Generated evaluation files:")
                for key, path in eval_results.items():
                    logger.info(f"  - {key}: {path}")
            except Exception as e:
                logger.error(f"Error running evaluation task: {e}", exc_info=True)

    logger.info("--- Standalone Evaluation Test Finished ---")
